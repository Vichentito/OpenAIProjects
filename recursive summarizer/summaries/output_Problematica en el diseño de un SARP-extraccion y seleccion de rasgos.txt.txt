El reconocimiento de patrones es un proceso que involucra la descripción concisa y representativa de los elementos del universo observado. Esta información incluye nombres, características, relaciones, modos de comportamiento, etc. El sistema de reconocimiento de patrones está compuesto por tres etapas: adquisición de datos sensoriales, extracción de características y toma de decisiones. El procesamiento de imágenes complejas se lleva a cabo a través de reconocimiento de patrones basado en atributos y reconocimiento de patrones basado en estructura. La clasificación se lleva a cabo mediante la regla del mínimo costo o el mínimo error. Existe un aprendizaje no supervisado que implica el análisis y agrupamiento de los datos para la inferencia de parámetros y la evaluación del desempeño del sistema.

El aprendizaje supervisado implica etiquetar automáticamente patrones de entrenamiento. Esto se logra a través de técnicas de agrupamiento o clustering, que detectan y agrupan enjambres de puntos con alta densidad de muestras en el espacio de observación. Para decidir si un punto pertenece o no a un agrupamiento, se necesita una medida de proximidad o similitud. La distancia Euclideana es la más comúnmente usada. Además, se requiere un criterio de agrupamiento para cuantificar cuando una partición es mejor que otra. Un ejemplo sencillo sería el algoritmo k-medias (/:-means), donde el objetivo es minimizar la suma total sobre el conjunto de entrenamiento de la distancia cuadrática de cada punto al vector valor medio de su agrupamiento.

El efecto de un cambio mínimo atómico en la configuración de agrupamientos se deduce considerando que un punto x se saca del agrupamiento X para pasarlo a otro agrupamiento X. Esta reasignación solo afectará los agrupamientos y r cuyos valores medios pasarán a ser fy = m + (juX) y ip = + (u, x) respectivamente. Para deducir la primera ecuación, se calcula el valor medio de X antes y después de la reasignación. Como resultado, se obtiene una identidad que se verifica andlogamente para la segunda ecuación. Para calcular el cambio global en el valor de J, hay que calcular los cambios en las contribuciones de J; y J,. Para el nuevo agrupamiento i-ésimo, el resultado es que MN - N-1 T = I- (mm x)" (ui x) = Ti - y y andlogamente para el agrupamiento r se obtiene Jp. = Ip +. Estos resultados permiten concluir que las diferencias en los patrones de una misma clase pueden deberse a ruido, deformaciones, variabilidad biológica, etc., por lo tanto es necesario asumir esta variabilidad en los patrones y el proceso asociado a la generación de patrones puede ser descrito adecuadamente mediante un modelo probabilístico. Por lo tanto, es conveniente dividir el problema del reconocimiento automático en etapas (sensor, extracción de características y clasificador). La función densidad de probabilidad condicional para la i-ésima clase viene dada por p(x|w;). La probabilidad a posteriori (una vez observado el patrón x) para cada clase w; viene dada por la Fórmula de Bayes.

En este artículo, se discuten formas de relacionar probabilidades condicionales para el diseño de un Sistema de Reconocimiento de Patrones (SRP). Se introduce una regla de decisión estadística conocida como Regla del Mínimo Costo, que asigna un patrón x a una clase w; si la probabilidad de que el patrón pertenezca a la clase w; es mayor que la probabilidad de que pertenezca a cualquier otra clase. También se presenta la Regla del Mínimo Error, que asigna x a la clase w; cuando la probabilidad de que x pertenezca a la clase w; es máxima. Estas reglas son aplicables si conocemos totalmente las características estadísticas del modelo en el proceso de generación de los patrones. Sin embargo, en la práctica surgen problemas tales como no poder conocer totalmente el modelo o tener restricciones económicas para diseñar el SRP óptimo.

Para el diseño de un sistema de reconocimiento de patrones (SRP), se asume generalmente que el conocimiento disponible es un conjunto de entrenamiento formado por observaciones etiquetadas o no. Si hay conocimiento experto sobre los datos, el diseño del SRP involucra tres etapas: inferencia del modelo a partir de los datos (aprendizaje), desarrollo de reglas de decisión prácticas y simulación y evaluación del rendimiento del sistema. Si se asume que las clases responden a distribuciones normales, la regla de decisión para minimizar el error se reduce a asignar el patrón x a la clase cuya media este más cercana en distancia inducida por la norma asociada a esa clase. En el caso particular en que las probabilidades a priori son uniformes y las matrices de correlación tienen determinante constante en las distintas clases, entonces la regla de decisión se simplifica a asignar el patrón x a la clase cuyo vector medio sea el más cercano según la distancia de Mahalanobis (regla de decisión por media más cercana). Si hay dos clases, entonces la ecuación F(x) = |x - m1|2 - |x - m2|2 + C2 = 0 define la superficie discriminante entre estas clases.

Esta superficie es cuadrática ya que su ecuación resulta x7 (Spt Ey t)x2x7 (By wy Sy "Hy) + (wT Dy wy, Hz Ly My + C2 Ci) = 0. TS CO Oo eon DO se cuadrático lineal constante 2.2.3. Caso Particular 3. En este caso asumimos que tenemos 2 clases con la misma covarianza m=2, %,=h,=5. Observando la ecuación para el caso anterior, vemos que se anula el término cuadrático y por lo tanto la superficie discriminante resulta lineal como función del patrón vectorial x. La superficie de separación es un hiperplano definido por: xD (py, fy) +ete=x wtete=0 siendo w= E(p, py). El resultado es un clasificador lineal se puede implementar como se muestra en la figura 5. Esta estructura es idéntica a la de una importante familia de máquinas lineales usadas en sistemas de decisión, entre las cuales podemos mencionar al Perceptrón. La inferencia de los parámetros 1; y; involucrados en las reglas de decisión es directa a partir de los conjuntos de entrenamiento {xj i=1,---,m;jf=1,---,N} con xj Eu;. Supongamos que disponemos de un conjunto de patrones vectoriales x; j = 1,--- ,N con sus correspondientes clases verdaderas +; conocidas. Es importante notar que este conjunto de patrones debería tener independencia estadística con el conjuntos de patrones de entrenamiento del sistema. Sean (3; las etiquetas asignadas a cada x; por el sistema de reconocimiento de patrones que estamos evaluando, e introduzcamos las variables aleatorias 7(x,) definidas según 0 siy=28 Qj) = { oe 1 si yj #3; Notar que el valor esperado de 1)(x) para un patrón x (2 elegido al azar es Elnl= [0-6 +0- [1 ~ eb) plxjax = [ ebopljax =e Q en tanto la varianza resulta E[(n-e)*] = [ e(x)p(x)dx e? = e(1e). Si hacemos N observaciones independientes de 7 y definimos la nueva variable aleatoria como N e=5 dn j=l encontramos que tiene distribución Binomial con valor medio e (como se ve calculando la esperanza) y por lo tanto es un estimador insesgado del error medio e. La desviación estándar de este estimador se calcula como oe = Var = (E(@] -2) Nie Sustituyendo y desarrollando resulta N N 1 1 , N-1 3 1 = yp E lnm] e = 5 F [n"] + ee wel e) j=l k=1 de donde la desviación del estimador será Resumiendo, hemos visto que podemos estimar el error medio de clasificación presentándole a nuestro sistema de clasificación un conjunto de patrones que pertenecen a clases conocidas. El error se estima contando el número de discrepancias entre la clase verdadera y la etiqueta de clase asignada por el sistema, y dividiendo finalmente este resultado entre el número de muestras en la prueba. Notar que si el error medio del sistema es pequeño, digamos de ~ 1%, vamos a necesitar un número grande de muestras para verificar este valor con una razonable confianza relativa. Por otra parte, hay situaciones en las que el sistema no dispone del conocimiento del experto o bien cuando el etiquetado individual es impracticable. En estos casos se presentan situaciones en las que el sistema deba diseñarse a partir del conjunto de patrones {a;; j = 1,2,--- , N} para los cuales no conocemos sus etiquetas correspondientes ;-. Resumen: Esta superficie cuadrática tiene su ecuación x7 (Spt Ey t)x2x7 (By wy Sy "Hy) + (wT Dy wy, Hz Ly My + C2 Ci) = 0. Se pueden inferir los parámetros involucrados directamente desde los conjuntos de entrenamiento {xj i=1,---,m;jf=1,---,N}. Para evaluar el desempeño del sistema se presentan patrones vectoriales con sus correspondientes etiquetas verdaderas conocidas con independencia estadística al conjunto usado para entrenar al sistema. El error se estima contando el número de discrepancias entre la clase verdadera y la etiqueta asignada por el sistema dividiendo finalmente este resultado entre el número total de muestras usadas para probarlo. Por otra parte hay situaciones en las cuales no hay etiquetas asociadas a los patrones usados para diseñar al sistema y en estas situaciones se usan técnicas llamadas aprendizaje no supervisado para diseñarlo.

El proceso de diseño requiere una primera etapa de análisis de las estructuras presentes en los datos de entrenamiento. Para un conjunto de entrenamiento suficientemente grande, se puede inferir la función densidad de probabilidad conjunta p(x) y recordar que mP(X) => P(wi)p(x|wi). Si la densidad conjunta es multimodal, cada uno de los modos debería corresponderse con la distribución condicional de cada una de las clases presentes. Por lo tanto, identificando estos modos en p(x) sería, en principio, posible particionar el espacio de observación en regiones disjuntas asociadas con cada una de las clases presentes. Si las distribuciones condicionales de cada clase son normales, se podrían recuperar los parámetros de cada distribución a partir del conjunto de entrenamiento. Con esto, se puede continuar con el diseño del clasificador como se vio en la sección anterior.