Universidad Nacional de Quilmes ofrece una cátedra de Visión Artificial en octubre de 2005 para presentar los pasos fundamentales de un proyecto de visión artificial. Esta técnica combina computadoras con cámaras de video para adquirir, analizar e interpretar imágenes como lo haría el ser humano. Esta tecnología es útil para inspecciones y supervisión, ya que es objetivo y consistente. Un sistema de visión artificial se compone fundamentalmente de una fuente de luz, un sensor de imagen, una tarjeta de captura o adquisición de imágenes, algoritmos de análisis de imagen y una computadora o módulo de proceso. La fuente de luz es extremadamente importante y la iluminación del entorno no suele ser aceptable debido a que pueden obtenerse imágenes con bajo contraste.

Los objetivos de una iluminación adecuada son proporcionar condiciones ambientales independientes y destacar los aspectos de interés de una aplicación. Existen varios tipos de iluminación como la iluminación posterior (backlight), frontal oblicua y direccional, frontal axial (difusa), de día nublado (CDI), de campo oscuro, arreglo de luces, y campo claro (brillante). Una estructura rígida se diseñó para integrar la fuente de luz, el difusor y la cámara. Para mejorar la difusión de la luz se pintó el interior con un color blanco mate. Se observó que esta primera estructura no brindaba una iluminación uniforme, así que se rediseñó eliminando las terminaciones en ángulos rectos. Se usó un lente de 4 mm que generó una deformación geométrica en la imagen, por lo que se reemplazó por un lente de 8 mm. La tercera estructura permitió reducir las deformaciones geométricas y fue rediseñada para reducir su tamaño. El software está compuesto por una lógica de control y análisis, además de una interfaz hombre-máquina. El software requiere determinar varios parámetros que son cargados automáticamente desde un archivo de configuración almacenado en el disco de la Smart-camera. Se usaron algoritmos de análisis de imagen contenidos en la librería mvIMPACT-SDK para procesar las imágenes.

Este documento presenta los aspectos del sistema de visión artificial, incluyendo dos diagramas que muestran los pasos necesarios para la captura de imágenes y la configuración del sistema. El primer diagrama muestra los pasos de compensación de iluminación, definición de zona de análisis, cálculo del histograma, cálculo del umbral óptimo y segmentación y procesamiento de la imagen. El segundo diagrama muestra la rutina de configuración. Para compensar la iluminación, se toma una imagen con fondo blanco y se realiza una compensación por software de la luminosidad de la imagen. La zona de análisis se delimita con el fin de descartar posible contaminación excluyendo zonas que excedan los límites de interés. El histograma determina el rango en el que se encuentra centralizada la mayor cantidad de pixeles. Para calcular el umbral óptimo se usa el método de Otsu. Luego, con el umbral óptimo se realiza la segmentación de la imagen etiquetando con cero los pixeles cuya intensidad sea menor al umbral óptimo y con 1 a los pixeles con niveles de gris mayores. El procesamiento de la imagen permite obtener los datos más exactos aplicando dos funciones: transformada morfológica open para suavizar los contornos y filtrado por área para eliminar objetos con áreas menores a un valor determinado. Finalmente, para crear la configuración se obtienen los parámetros esenciales del patrón: área, factor de forma y coordenadas del rectángulo perimetral. Estos parámetros otorgan una gran confiabilidad requiriendo un tiempo reducido para su procesamiento.

Los pixeles blancos de un objeto se multiplican por el valor del área del pixel. Los agujeros no contribuyen para el área. El factor de forma (ROUNDNESS) es un parámetro derivado del perímetro y el área de un objeto, con un rango de valores entre 0 y 1. Un objeto circular tendrá el máximo valor del rango (1), y para objetos irregulares o alargados el parámetro tiende al mínimo valor del rango (0). Las coordenadas del rectángulo perimetral (EXTREME BOX) especifican la máxima y la mínima coordenada de un objeto (bounding box). Los parámetros extraídos del análisis de la imagen patrón son guardados en un archivo en el disco de la cámara con extensión txt, incluyendo el número de alvéolos, el área mínima y máxima, el factor de forma mínimo y máximo, el valor del umbral, la bandera de descarte por comprimido fallado, ausente o manchado, el tipo de fondo, las coordenadas del rectángulo perimetral de cada alvéolo, el número de calles y la posición de las calles. El valor del umbral se calcula automáticamente y es un parámetro altamente afectado por las variaciones en la iluminación. Por lo tanto, es importante que el sistema implementado permita un aislamiento adecuado del sistema a las variaciones externas de la luz y que otorgue una iluminación constante y uniforme sobre la zona de análisis. La rutina Analizar incluye pasos como cargar la configuración, detectar el pulso, compensar la iluminación, segmentar la imagen, definir la zona de análisis por alvéolo e implementar la función control.

Subrutina Control es un proyecto desarrollado para realizar un análisis y determinar el estado correcto de los comprimidos en un blister. Si se detecta un comprimido defectuoso, se genera automáticamente una señal de descarte. El proyecto constó de varias etapas, como el análisis de las características técnicas de la Smart-camera mvBlueLINX, la configuración del puerto serial RS 232 para la comunicación entre la Smart-camera y la PC, la instalación y configuración de programas y librerías requeridas para el desarrollo y compilación de los programas en la PC, el estudio de la librería mvIMPACT-SDK, el programar en Visual C++, la prueba y ajuste del funcionamiento con la función import, el estudio de la librería GNU-PicoGUI, el desarrollo de una interfase hombre-máquina, el acople a las rutinas de configuración y análisis, el diseño y armado de una primera estructura de iluminación con su respectivo cálculo del lente adecuado para las dimensiones definidas, el modificar el software para capturar imágenes por detección de señal digital emitida por un sensor, montaje de mesa con todos los elementos del sistema integrado, pruebas y ajustes en planta con resultados muy buenos en detección de comprimidos ausentes, manchados y calados (aunque presentó algunos casos donde no detectó comprimidos partidos), diseño mejorado para integración mecánica con la línea de blisteado e inclusión de mecanismo para montaje de Smart-camera. Finalmente se realizaron pruebas finales en mesa con sistema terminado.

El sistema de segmentación de la imagen utiliza un umbral para determinar los límites de los objetos. El factor que afecta a la detección de estos casos es la holgura o falta de ella entre el comprimido y el alvolo, lo que dificulta la determinación de un umbral único adecuado para la correcta segmentación de toda la imagen. Por lo tanto, se tuvo que proceder a compensar por software el efecto de la iluminación para evitar que el cálculo automático del umbral se viera afectado. La Smart-camera mvBlueLYNX tardó aproximadamente 1.5 segundos en realizar un análisis completo, siendo el tiempo máximo admisible para el análisis de 1 segundo. Durante la prueba se forzaron diferentes casos de comprimidos defectuosos, por lo que el sistema presentó tan solo 2 fallas en la detección de un total de 640 comprimidos analizados (Tasa de Error: 0,31%).

La tecnología de visión artificial ha avanzado significativamente en los últimos años. Esto ha permitido el desarrollo de sensores y tarjetas de captura que pueden realizar una adquisición eficiente de imágenes para condiciones de iluminación exigentes y velocidades de trabajo muy altas. Estos sensores incluyen los TDI (time delay integrated) que utilizan varias líneas de captura sobre la misma línea del objeto para aumentar la sensibilidad, así como los sensores de área con resoluciones habituales de 1024x1024, aunque hay algunos modelos con hasta 3072x2048. Estos dispositivos también tienen una cuantización (conversión análogo-digital) que determina el número de bits usados para representar la información capturada. Además, existen tarjetas de captura que permiten transferir la imagen de la cámara a la memoria de la computadora. Estas tarjetas tienen una variedad de características importantes, como velocidad de transmisión, formato de los datos, profundidad de pixel, capacidad de captura por disparo, capacidad de preprocesado y velocidad de transferencia. Algunos dispositivos incluso tienen FPGA programables y microprocesadores incorporados para procesamiento en tiempo real.

La captura de imágenes con una señal externa ha mejorado con el reset asíncrono, lo que permite el inicio de la captura sin esperar al siguiente vertical blank. El control de la cámara con salidas RS-232 de la tarjeta facilita la variación de los parámetros de la cámara, esencialmente la ganancia de los canales RGB o de ópticas motorizadas en tiempo real. También se puede trabajar con multitap, existiendo tarjetas capaces de recibir información a través de varios canales a velocidades muy altas. En cuanto a la capacidad de proceso, sólo existe la posibilidad de incorporar un Procesador Digital de Señal (DSP), que permite realizar determinados algoritmos de análisis de imagen en la propia memoria, acelerando así la implementación. El desarrollo de módulos altamente especializados ha sido abandonado debido a la rapidez con que los procesadores convencionales mejoran sus prestaciones. Finalmente, se han integrado tarjetas de captura, visualización y procesamiento en un solo elemento para trabajar en entornos industriales. Los algoritmos relacionados con visión artificial son variados y abarcan técnicas y objetivos distintos. Estos pasos se pueden dividir en filtrado o segmentación, cálculo de preprocesamiento, selección y diseño del clasificador para llegar a la implementación y clasificación. Cuando se adquiere una imagen mediante sistema de captura, suele estar contaminada por ruido, mala iluminación o bajo contraste, por lo que es necesario realizar un preprocesamiento para corregir estos problemas y transformaciones para acentuar las características deseadas.

El Sistema de Visión Artificial se usa para tomar decisiones a partir de la información capturada por una cámara. Estas decisiones se basan en transformaciones geométricas, transformación del histograma, filtrado espacial y frecuencial, segmentación o aislamiento de los objetos de interés, extracción y selección de características y reconocimiento de formas e inteligencia artificial. La transformación geométrica corrige la perspectiva y reconstruye los objetos en 3D. La transformación del histograma modifica el contraste y el rango dinámico de los niveles de gris. El filtrado espacial y frecuencial suele ser pequeño para no modificar la imagen original. La segmentación aísla los objetos de interés de la escena. La extracción y selección de características extrae medidas que caractericen adecuadamente a los objetos. Por último, el reconocimiento de formas e inteligencia artificial toma decisiones a partir de la información capturada.

El sistema de adquisición de imágenes recopila información sobre las transformaciones y operaciones realizadas con ellas. La información extraída se puede considerar como un vector que contiene características o rasgos diferenciadores de la imagen analizada. Estos vectores contienen todas aquellas medidas necesarias para una aplicación de medición, o en el caso de aplicaciones de inspección o clasificación, los datos con los que trabaja un reconocedor o clasificador para extraer conclusiones a partir del vector de entrada. Es necesaria una etapa de selección de características y una etapa de aprendizaje o entrenamiento para el diseño del clasificador. En el reconocimiento de formas aplicado a la visión artificial se utilizan técnicas como el aprendizaje supervisado, algoritmos de clasificación no supervisados o clustering y redes neuronales. Los altos requerimientos computacionales para estas técnicas son evidentes al considerar un ejemplo como el reconocimiento automático de matrículas, que requiere 133 MIPS para trabajar en tiempo real. Quizás sea este el campo que más rápidamente ha evolucionado debido al uso generalizado de este tipo de sistemas.

La rápida evolución de los procesadores de propósito general ha llevado a la utilización de estos procesadores en lugar de los procesadores especializados (más caros y con una evolución más lenta), excepto para casos muy específicos. La potencia de los procesadores actuales, junto con las arquitecturas hardware y software que soportan los sistemas multiprocesadores, permiten el desarrollo de algoritmos complejos con tiempos de respuesta muy cortos. Entre las mejoras destacadas se encuentran: el Pipeline, Tecnologías de ejecución dinámica, Juegos de instrucciones SIMD (Single Instruction, Multiple Data), Cachés de 2 nivel y Unidades de ejecución en coma flotante FPU avanzadas. Además, para aplicaciones con requerimientos computacionales muy altos y estrictos requerimientos de tiempo, la existencia de hardware especializado o el uso de otras arquitecturas paralelas que hagan uso de DSP o sistemas FPGA aseguran la viabilidad. Por último, se han desarrollado librerías software que implementan numerosas funciones utilizadas habitualmente en esta tecnología, lo que permite reducir el ciclo de desarrollo y facilitar la obtención e implementación de algoritmos complejos con tiempos adecuados. Los sistemas de respuesta son automatismos que responden electromecánicamente para corregir o evitar problemas en los sistemas de producción, además generan estadísticas e informes del proceso. Entre los dispositivos electromecánicos utilizables destacan los variadores de frecuencia para el control de motores y servomotores.

La comunicación entre PCs y los autómatas programables que controlan distintos dispositivos a través de redes de campo como Profibus o WorldFIP, y más recientemente mediante Industrial Ethernet y protocolos Wireless, ha permitido la integración de los sistemas de visión y sus acciones de respuesta dentro de la estructura CIM de las empresas, lo que supone un ahorro económico. Como ejemplo, se presenta el proyecto final de carrera realizado por la egresada de la Universidad Nacional de Quilmes, Iris Herrero, el cual consiste en el desarrollo de un sistema de visión para la inspección y supervisión de blisters con base de PVC o aluminio. El sistema está compuesto por un sensor, una CPU, un lente, un software y una estructura de iluminación y montaje. En particular, se eligió la Smart-camera mvBlueLYNX como elemento sensor, además de la librería mvIMPACT-SDK para el procesamiento y análisis de imágenes, y la librería GNU-PicoGUI para programar la interfaz hombre-máquina. Para resaltar los detalles relevantes se utilizó luz blanca difusa como iluminación con arrays de LEDs blancos y técnicas de iluminación frontal mediante difusión y reflexión.