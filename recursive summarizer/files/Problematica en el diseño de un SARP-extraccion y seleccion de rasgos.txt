Reconocimiento de Patrones

J. Kittler (revisado y ampliado por GTI-IIE) Revision:0.9, Fecha: 1/09/2002

Notas del seminario de Reconocimiento de Patrones de Grupo de Tratamiento de Imagenes del
Instituto de Ingenieria Eléctrica, basado en las notas del curso del Prof. J. Kittler en la Univ. de
Surrey.

Indice
1. Modelo de Sistema de Reconocimiento de Patrones 1
1.1. Introduccifn. . 2... ee 1
1.2. Modelo de Sistema de Reconocimiento de Patrones ................ 00000. 1
1.3. Modelo del Proceso de Generacion de Patrones .. 2... 2 ee ee 2
1.3.1. Modelo Probabilistico 2... 0... ee 2
1.3.2. Relaciones Baésicas . 2... ee 2
1.3.3. Ejemplo: Un problema de reconocimiento de caracteres ............0.. 3
1.4. Reglas de Decision Estadistica . 2... ee 3
1.4.1. Regladel Minimo Costo .. 2... 0.00.02 0000002 eee ee 3
1.4.2. Regladel Minimo Error . 2... ee 4
2. Disefio de un Sistema de Reconocimiento de Patrones 5
2.1. Problemas en el Disefio de un Sistema de Reconocimiento de Patrones............ 5
2.2. Reglas de Decision para Clases con Distribucién Normal (Gaussiana) ............ 5
2.2.1. CasoParticularnl 2... ee 6
2.2.2. CasoParticular2 . 2... 7
2.2.3. Caso Particular3 2... 7
2.2.4. Inferencia de los pardmetros ... 2... ee 7
2.3. Evaluacién del Desempefio de un Sistema de Clasificaci6n ..............000. 8
3. Apendizaje no supervisado 9
3.1. Aprendizaje no supervisado y andlisis de agrupamientos ................00-. 9
3.2. Medidas de Similitud y Criterios de Agrupamiento ..................000. 10

3.3. Algoritmo de k-medias (k-means)... 2... 0 ee 11

 NEW PAGE 
1. Modelo de Sistema de Reconocimiento de Patrones 1

1. Modelo de Sistema de Reconocimiento de Patrones

1.1. Introduccién

El objetivo del procesamiento e interpretacidn de datos sensoriales es lograr una descripcidn concisa y repre-
sentativa del universo observado. La informacion de interés incluye nombres, caracteristicas detalladas, rela-
cionamientos, modos de comportamiento, etc. que involucran a los elementos del universo (objetos, fenédme-
nos, conceptos)

Estos elementos se perciben como patrones y los procesos que llevan a su comprensi6n son llamados procesos
perceptuales. El etiquetado (clasificacién, asignacién de nombres) de esos elementos es lo que se conoce
como reconocimiento de patrones. Por lo tanto, el reconocimiento de patrones es una herramienta esencial
para la interpretacién automatica de datos sensoriales.

El sistema nervioso humano recibe aproximadamente 10° bits de datos sensoriales por segundo y la mayoria
de esta informaci6n es adquirida y procesada por el sistema visual. Andlogamente, la mayoria de los datos a
ser procesados automaticamente aparecen en forma de imagenes.

El procesamiento de imagenes de escenas complejas es un proceso en multiples niveles que se ilustra en la
figura 1 mostrando la participacion relativa de los dos tipos de metodologias necesarias:

= Reconocimiento de patrones basado en atributos.

= Reconocimiento de patrones basado en la estructura.

Hiatal I

Pixel Segmentos Primitivas Objetos Grupos Escena
de Objetos de Objetos

oy Informacién Relacional

Informaci6n de Atributos

Figura 1: Distintos tipos de informacién usada en diferentes niveles de procesamiento.

1.2. Modelo de Sistema de Reconocimiento de Patrones
Los procesos perceptuales del ser humano pueden ser modelados como un sistema de tres estados:

= adquisicion de datos sensoriales
= extracci6n de caracteristicas

= toma de decisiones

 NEW PAGE 
10 INDICE

MY To Ts

Figura 6: Distribucion conjunta multimodal y regiones asociadas a cada clase..

directamente las regiones I’; lo cual es suficiente para nuestros intereses ya que esto puede usarse directa-
mente para la clasificacién de nuevos datos simplemente usando el criterio:

Asignarxaw; <=> x€Tj

Un opci6n alternativa seria usar este u otro criterio para clasificar los patrones en el conjunto de entrenamiento
y luego usar estas etiquetas para disefiar el sistema de reconocimiento de patrones usando un aprendizaje
supervisado. En la practica ocurre que determinar explicitamente las regiones I’; implicaria estimar la funcién
de densidad conjunta y luego analizarla en un espacio de dimensién n lo que generalmente es impracticable
por su complejidad computacional. Ademas como vimos, solo necesitamos de un método indirecto que nos
permita etiquetar automaticamente los patrones de entrenamiento. Entonces lo que queremos es alguna forma
de hacer una particién del conjunto de entrenamiento en clases con una misma etiqueta y esto es lo que se
conoce como métodos de agrupamiento 0 clustering.

Intuitivamente podemos anticipar que las modas en la funcién de densidad conjunta p(x) estaran asocia-
das a regiones con alta densidad de muestras en el espacio de observacion. El proposito de las técnicas de
agrupamiento sera justamente detectar y agrupar estos enjambres de puntos.

3.2. Medidas de Similitud y Criterios de Agrupamiento

El proposito de los métodos de agrupamiento sera analizar y extraer la estructura presente en un conjunto de
patrones o muestras de entrenamiento. Diremos que un conjunto de datos esta bien estructurado si contiene
varios enjambres de patrones cercanos entre si, o sea regiones de alta densidad, separados por otras regiones
relativamente vacias 0 con poca densidad.

Vemos que los puntos de un mismo agrupamiento apareceran mas proximos entre ellos que a puntos en otros
agrupamientos. Esta observaci6n nos lleva a concluir que si queremos decidir si un punto x pertenece o no a
un agrupamiento necesitaremos una medida de proximidad o similitud. Se han sugerido y estudiado un gran
numero de tales medidas, pero probablemente las més comunmente usadas son las medidas de distancia y en
particular la distancia Euclideana.

La afinidad de un punto a un agrupamiento se puede determinar ya sea midiendo su similitud con otros
puntos en el agrupamiento o bien con un modelo definido para el agrupamiento. El ejemplo mas sencillo de
esto ultimo es representar un agrupamiento 7 por su vector medio j1;; en este caso la afinidad entre un puntox
y el agrupamiento se puede cuantificar con la distancia Euclideana al cuadrado

d(x, pi) = [(x _ Hi)” (x - Hi)

 NEW PAGE 
3. Apendizaje no supervisado 11

@ c) ®@ @
e 4 ®
® @ e
e © ® °°
®
@ ®® &®@
®
®
® @ o ®
® ®
° ® ® ®
° ® ee ©
® ® ®
®
e °e
° ®
® ® ®
® 8 ®
o °
®
® ® s
® ®
® ®
® ee
eo ee
@®

Figura 7: Datos structurados vs. no estructurados.

Pero para particionar un conjunto de puntos en agrupamientos de una manera 6ptima no nos alcanza con una
medida de afinidad o similitud sino que ademas necesitamos algun criterio de agrupamiento que nos permita
definir cuantitativamente cuando una particién es mejor que otra. Obviamente tanto el criterio de agrupa-
miento que definamos tanto como el algoritmo de agrupamiento asociado, estaran intimamente relacionados
con la medida de similitud usada y se definiran a partir de esta.

En la siguiente seccién veremos algunos ejemplos de métodos de agrupamiento que se basan en los conceptos
anteriores.

3.3. Algoritmo de k-medias (/:-means).

Supondremos que el conjunto de datos X contiene k agrupamientos y que cada uno de estos subconjuntos
XX; puede representarse adecuadamente con su valor medio j1;. Como se menciona anteriormente, en este
caso podemos usar la distancia Euclideana como una medida de similitud. Se deduce que un criterio de
agrupamiento adecuado en este caso es considerar la suma total sobre el conjunto de entrenamiento de la
distancia cuadratica de cada punto al vector valor medio de su agrupamiento.

El objetivo del algoritmo de agrupamiento sera encontrar entre todas las particiones de X en k conjuntos
{X;; i=1,2,---,k} aquella que minimice el criterio de agrupamiento elegido.

Dicho formalmente, queremos encontrar los agrupamientos {X_;} que minimizan la funcién

k k ON;

J= SOK = SLY a xij, mi) siendo xij € Xi, Ni = #X:

i=l i=1 j=l
entre todas las posibles particiones de X en k subconjuntos.

Un algoritmo para minimizar J puede deducirse considerando el efecto de un cambio minimal 0 atémico en
la configuracién de agrupamientos, que consiste en sacar un punto x que este en el agrupamiento X, para
pasarlo a otro agrupamiento X,..

 NEW PAGE 
12 INDICE

Claramente esta reasignacion afectara solo a los agrupamientos | y r cuyos valores medios pasaran a ser

fy =m + ——(ju—X) Yip = + (u, — x)
fa = MX) YB = be ~ (Her —

1
N-1
respectivamente.

Para deducir la primera ecuaci6n calculamos el valor medio de X; antes y despues de la reasignacién

1 Ni 1 Ni-1 1 Ni
may = Ls Tl Ls *
j=l j=l j=l
donde hemos asumido que el punto reasignado es el ultimo en la sumatoria. De aqui resulta que
N, 1 1
(N; — 1)f = Ni — x i Noi No1* fi = wi 4 Noi! x)

y andlogamente se verifica la segunda identidad.

Por lo tanto para calcular el cambio global en el valor de J bastard calcular los cambios en las contribuciones
de J; y J,. Para el nuevo agrupamiento |-esimo tendremos

Ni-1 N-1
= d(xj, mu;) = > (xj — 1)" Oy — wr) =
j=l j=l
a [-X\T MW - xX ba 1 - x
= — 1 LT — 1 ‘LT — — 1 — — 4 LT
Solas —net AY me RM) — ene RAI en BF)
2 ~ M T NP T
Ti Mo z(m - ») Debs - Bi) +O ya Mt — x)" (ui — x) + w—p" — x)" (ui — x)
oe

de donde luego de agrupar concluimos que

MN
N-1

T= I- (mm — x)" (ui — x) = Ti -
y andlogamente para el agrupamiento r se obtiene

Jp. = Ip +


 NEW PAGE 
2 INDICE

Universo Patron Extractor | Caracteristicas _ Decisién
Sensor -——— de [>| Clasificador-
(Objetos, Conceptos) Representacion| Caracteristicas

Figura 2: Etapas en un sistema de reconocimiento de patrones.

Por lo tanto es conveniente dividir el problema del reconocimiento automatico de una manera similar

Sensor Su propésito es proporcionar una representaci6n feasible de los elementos del universo a ser clasifi-
cados. Es un sub-sistema crucial ya que determina los limites en el rendimiento de todo el sistema.

Idealmente uno deberia entender completamente las propiedades fisicas que distinguen a los elementos
en las diferentes clases y usar ese conocimiento para disefiar el sensor, de manera que esas propiedades
pudieran ser medidas directamente. En la practica frecuentemente esto es imposible porque:
= no se dispone de ese conocimiento
= muchas propiedades titiles no se pueden medir directamente (medici6n no intrusiva)
= no es econdmicamente viable
Extracci6n de Caracteristicas Esta etapa se encarga, a partir del patron de representacion, de extraer la

informacion discriminatoria eliminando la informacion redundante e irrelevante. Su principal propdésito
es reducir la dimensionalidad del problema de reconocimiento de patrones.

Clasificador El la etapa de toma de decisiones en el sistema. Su rol es asignar a la categoria apropiada los
patrones de clase desconocida a priori.

1.3. Modelo del Proceso de Generacién de Patrones
1.3.1. Modelo Probabilistico

Las diferencias en los patrones de una misma clase puede deberse a ruido, deformaciones, variabilidad
bioldgica, etc. Por lo tanto debemos asumir esta variabilidad en los patrones y el proceso asociado a la
generacion de patrones puede ser descripto adecuadamente mediante un modelo probabilistico.

De lo anterior podemos asumir que cada patrén
x= [01,22,--- an]

es un vector aleatorio n-dimensional perteneciente a una de m posibles clases w; i = 1,--- ,m donde cada
clase w; tiene una probabilidad de ocurrencia a priori igual a P(w;). La distribucién de probabilidad del
vector patrén x de la clase w; se caracteriza por la funcién densidad de probabilidad condicional para la
i-esima clase p(x|w;).

1.3.2. Relaciones Basicas

Notar que las probabilidades a priori de las clases suman uno, 0 sea

m

> P(wi) =1
i=l

La densidad conjunta o funci6n de densidad de probabilidad no condicional p(x) vienen dada por

m

px) = > p(xlui) Pui).

i=1

 NEW PAGE 
1. Modelo de Sistema de Reconocimiento de Patrones 3

En la practica nos interesa calcular la probabilidad a posteriori (una vez observado el patrén x) para cada
clase w;, la cual viene dada por la Férmula de Bayes que relaciona probabilidades condicionales segtin:

p(x|wi)Pwi) p(x|wi)P(wi)
p(x) Wyh1 P(x|w;)P(w;)

P(wi|x)

1.3.3. Ejemplo: Un problema de reconocimiento de caracteres

= m= 26 ntmero de diferentes caracteres excluyendo los digitos.
= n=8_ ntmero de medidas

= a, i= 1---n distancia entre el centro de gravedad y el punto de interseccién mas lejano en la

. . P (i=1)a Lt
semirrecta con origen en este centro y formando un Angulo ~~~ con el eje Ox.

« P(w;)  probabilidad a priori de la ocurrencia del i-ésimo caracter en un lenguaje dado.

Figura 3: Atributos en reconocimiento de caracteres.

1.4. Reglas de Decision Estadistica
1.4.1. Regla del Minimo Costo

Dadas las caracteristicas del modelo probabilistico adoptado para el proceso de generacion de patrones; cémo
decidimos a que clase asignar el patr6n x observado?

Para resolver este problema, definamos un costo de decisi6n pj; 1 < i,7 <n asociado con la decisién de
asignar a la clase w; un patrén x que pertenece a la clase wj.

Notar que
epi; costo de una decision correcta, en general se define como 0

epi; esen general diferente de pj;

© pij 20

Ejemplo: Verificacién de Firmas

En una aplicacién de deteccién de firmas falsas tendriamos en principio dos clases

w, < la firma es auténtica
W2 < la firma ha sido falsificada

Claramente en este contexto podemos cometer 2 tipos de errores de clasificacién que sin embargo implican
costos muy diferentes; de modo que se debera cumplir p21 > pi2.

 NEW PAGE 
4 INDICE

Denotaremos con I’; la regidn del espacio de observacién 2 tal que nuestra regla de decisiOn asigna
x>w VxeEeT;

o sea que la region I’; esta asociada con la clase w;.

El costo medio de clasificar un patrén x € I; como perteneciente a la clase w; es

m

r(x) = > pi P(wilX)

i=1

Por lo tanto el costo para toda la region I’; se obtiene integrando sobre todos los valores posibles con sus
correspondientes probabilidades de observaci6n:

R; -[ rj (x)p(x)dx

a

Finalmente el costo total de nuestro sistema de decisién viene dado por

m m

R= VOR = >| (x psPlain) p(x)dx
j=l j=" Viet

De modo que podemos concluir que el costo total sera minimizado si el espacio de observaci6n se particiona
de manera tal que si x € Tj se tenga

m

Se pis Pwilx) < SO pinPwilx) VE AG
i=1 i=1

Hemos llegado por lo tanto a la llamada Regla de Decisién de Minimo Costo de Bayes, que establece
m m
Asignar x > w, <=> Se pijp(%|wi)P(wi) = min » PikP(X|w;i) P(wi)
i=l

1<k<m
i=l ~~

donde en en la tiltima ecuacién hemos usado la identidad de Bayes: P(w;|x)p(x) = p(x|w;)P(wi).

1.4.2. Regla del Minimo Error

Consideramos ahora un modelo de costos cero-uno, 0 sea

pi =O Visicm
pig =l Visiggm,iFj

En este caso el lado derecho de la regla del minimo costo queda

m

> pik P(wilx) = > P(wi|x) = 1 — P(we|x)
i=l 1<i<m
ifk

y la regla de decisidn correspondiente resulta:

Asignar x > w;, <=> = P(w;|x) = max P(w,|x)

1<k<m
Observar que si asumimos que se asigna x — w,, la probabilidad condicional de error e(x) viene dada por

e(x) = 1 — P(w;|x).

 NEW PAGE 
2. Disefio de un Sistema de Reconocimiento de Patrones 5

Por lo tanto el error condicional ser4 minimo si la clase w; se elije usando la Ultima regla de decision. En este
caso el error medio

se conoce como error de Bayes.

Integrando en cada regién de decisién este error se puede descomponer como

m m

e=1- X(, P(w;|x)p(x)¢x) =1- rrwn(f p(x|w)¢x) .

2. Disefio de un Sistema de Reconocimiento de Patrones

2.1. Problemas en el Disefio de un Sistema de Reconocimiento de Patrones

Si conociéramos totalmente las caracteristicas estadisticas del modelo en el proceso de generacién de los
patrones, esto es si supiéramos P(w;), p(x|w;) Vi; entonces podrfamos disefiar el sistema de reconocimiento
de patrones (SRP) 6ptimo mediante aplicaci6én directa de la teorfa de decisién de Bayes. Sin embargo en la
practica surgen los siguientes problemas:

= el modelo no se puede conocer totalmente y/o

= lacomplejidad del SRP a disefiar esta restringida por consideraciones econémicas (hardware, tiempo)

Por lo general la base de conocimiento disponible para el disefio de un SRP es un conjunto de entrenamiento
constituido por observaciones, ya sea etiquetadas o no.

En el primer caso asumimos que para cada patrén o vector de observaciones x; i = 1,2,---,N enel
conjunto de entrenamiento, un experto asigna una etiqueta con la clase correcta 7;. El disefio de un sistema
basado en un conjunto de datos clasificados de antemano se conoce como aprendizaje supervisado.

Si no se dispone de conocimiento experto sobre el conjunto de datos, o si el etiquetado de los patrones de
entrenamiento es impracticable por razones practicas; entonces el problema de disefio implica la necesidad
de una primera etapa de andlisis de los datos. Este proceso primario de andlisis se conoce como una etapa de
aprendizaje no supervisado.

Por lo tanto, en el caso general, dado un conjunto de entrenamientos el disefio del SRP implica:

1. Inferencia del modelo a partir de los datos (aprendizaje).
2. Desarrollo de reglas de decisién practicas.

3. Simulacién y evaluacién del rendimiento del sistema.

2.2. Reglas de Decision para Clases con Distribucién Normal (Gaussiana)

Supongamos que las clases responden a distribuciones normales, 0 sea que las densidades de probabilidad
condicionales de cada clase tienen la forma

1 1
p(x|w;) = ———s exp(—5 (x — mi) EP 1(x- pi)") Vis lym.
(27) 2 ||? 2

siendo

(e = E [x|u] vector medio para la i-ésima clase

&j = Cov[x|w;] = E [(x — w;)(x — ,)* |wi]] matriz de covarianza para la i-ésima clase

 NEW PAGE 
6 INDICE

Si tomamos logaritmo a ambos lados de la regla de Bayes del minimo error obtenemos:

Asignar xX > wy; 9 <=>

log P(w;) — = (nlog(2m) + log |Ey| + (x — Mj) D5 (x — H;)") =

Nile

: 1 . -1 T
pmax, flog Pw) — 5 (mlog(2m) + log [Bel + (x — Ha) BRC Ma)") | CA)

Multiplicando por 2, sacando para afuera de la expresién el signo de menos y agrupando los términos que no
dependen de x resulta:

Asignar x > wy; <=> (x = py) E71 (x — py)" — Oj = min [(x — py) Bq (x — py)” — Cy]

siendo
Cy = 2 log P(wx) — (nlog(27) + log |E,|)

Observar que en el caso particular en que las probabilidades a priori son uniformes y las matrices de correla-
cidn tienen determinante constante en las distintas clase, de modo que

1
P(wr) = —VI<k<m y  log|E,| =log|Ee|Vli<k Ak’ <m
m
la regla de Bayes del minimo error se simplifica a:

Asignar xy > (x= )EP MK pay)” = min [Ox — wy) EE He)"

Si definimos para cada clase una norma a partir de su matriz de covarianza como sigue
Vx EQ => ||x||2=xBpix? L<k<m

podemos escribir el criterio anterior como:

Asignar x > wj Ix — ayl|, = min |x — Helle

De modo que asignamos un patron x arbitrario a la clase w, tal que su vector medio j2; este mds cercano en

la distancia inducida por la norma asociada a esa clase.

2.2.1. Caso Particular 1

Supongamos que las probabilidades a priori y las covarianzas son constantes en las clases

B= Ui=T y Plwi)=P(wj) VisiAjsm

Definimos la norma y distancias asociadas a & como antes:
Ix? =xExT s  d(x,x’)=|x-x'|| Vx,x’ €Q

Esta distancia se conoce como distancia de Mahalanobis asociada a la covarianza 4. Notar que en el caso
particular que & = I esta distancia coincide con la distancia cuadratica Euclidiana.

Por lo tanto la regla del minimo error se reduce a asignar el patr6n x a la clase cuyo vector medio sea el mas
cercano segtin la distancia de Mahalanobis (Regla de decisién por media mds cercana):

Asignar x > w; <=> d(x, ;) = ayn d(x — py)

 NEW PAGE 
2. Disefio de un Sistema de Reconocimiento de Patrones 7

h
« Hy
Q
: “d(X, Hy)
- —— =
d(x, bs). ee
y yg Hy
Hs

Figura 4: Regla de asignacién por media mas cercana.

2.2.2. Caso Particular 2
Ahora analizaremos el problema general de decisién entre dos clases
> m=2, AD.
Aplicando la regla de Bayes vemos que la ecuacién
F(x) = |x = Mall, — Il — Melly + C2 — Ci =0

define la superficie de separacién, conocida como superficie discriminante, entre las regiones asociadas a
cada una de las clases w1 y w2. En general esta superficie es cuadratica ya que su ecuacion resulta:

x7 (Spt — Ey t)x— 2x7(By wy — Sy "Hy) + (wT Dy wy, — Hz Ly My + C2 — Ci) = 0.
TS CO — Oo eon DO se

cuadratico lineal constante

2.2.3. Caso Particular 3

En este caso asumimos que tenemos 2 clases con la misma covarianza

m=2, %,=h,=5

Observando la ecuaci6n para el caso anterior, vemos que se anula el término cuadratico y por lo tanto la
superficie discriminante resulta lineal como funci6n del patron vectorial x. La superficie de separacién es un
hiperplano definido por:

xD (py, — fy) +ete=x wtete=0 — siendo w= E(p, — py).

El resultado es un clasificador lineal se puede implementar como se muestra en la figura 5.

Esta estructura es idéntica a la de una importante familia de maquinas lineales usadas en sistemas de decision,
entre las cuales podemos mencionar al Perceptrén.

2.2.4. Inferencia de los parametros

La inferencia de los pardmetros 1; y ©; involucrados en las reglas de decisién es directa a partir de los
conjuntos de entrenamiento

{xj i=1,---,m;jf=1,---,N} con xj Eu;.

 NEW PAGE 
8 INDICE

x'w + cte =0

Figura 5: Regla de asignacién por media mas cercana.

El patron vectorial medio para la clase w; se puede estimar como

1 &
A= wy

en tanto la matriz de covarianza se estima con

2.3. Evaluacién del Desempeiio de un Sistema de Clasificacién

Supongamos que disponemos de un conjunto de patrones vectoriales x; j = 1,--- ,N con sus corres-
pondientes clases verdaderas +; conocidas. Es importante notar que este conjunto de patrones deberia tener
independencia estadistica con el conjuntos de patrones de entrenamiento del sistema.

Sean (3; las etiquetas asignadas a cada x; por el sistema de reconocimiento de patrones que estamos evaluan-
do, e introduzcamos las variables aleatorias 7(x,;) definidas segin

0 siy=28
Qj) = { oe
1 si yj #3;

Notar que el valor esperado de 1)(x) para un patr6n x € (2 elegido al azar es

Elnl= [0-6 +0- [1 ~ eb) plxjax = [ ebopljax =e

Q

en tanto la varianza resulta
E[(n-e)*] = [ e(x)p(x)dx — e? = e(1—e).
2
Podemos también deducir esto observando 7 es una variable aleatoria de Bernoulli tal que la probabilidad

p{n = 1} es igual a la probabilidad media e de error del sistema.

Si hacemos N observaciones independientes de 7 y definimos la nueva variable aleatoria € como

N

e=5 dn

j=l

 NEW PAGE 
3. Apendizaje no supervisado 9

encontramos que é tiene distribucién Binomial con valor medio e (como se ve calculando la esperanza) y por
lo tanto é es un estimador insesgado del error medio e.

La desviacién estandar de este estimador se calcula como

oe = Var = (E(@] -2)

Nie

Sustituyendo é y desarrollando resulta

N N
1 1 , N-1 3 1
= yp » » E lnm] —e = 5 F [n"] + ee wel e)

j=l k=1

de donde la desviacién del estimador sera

Resumiendo, hemos visto que podemos estimar el error medio de clasificacién e presentandole a nuestro sis-
tema de clasificacidn un conjunto de patrones que pertenecen a clases conocidas. El error se estima contando
el nimero de discrepancias entre la clase verdadera y la etiqueta de clase asignada por el sistema, y dividiendo
finalmente este resultado entre el ntiimero de muestras en la prueba.

Notar que si el error medio del sistema es pequefio, digamos de ~ 1%, vamos a necesitar de un nimero
grande de muestras de prueba para verificar este valor de desempefio con una razonable confianza relativa.

3. Apendizaje no supervisado

Es comtn encontrarse con situaciones en las que el sistema de clasificacién de patrones debe disefiarse par-
tiendo de un conjunto de patrones de entrenamiento {a;; j = 1,2,--- , N} para los cuales no conocemos
sus etiquetas de clase ¥;-

Estas situaciones se presentan cuando no disponemos del conocimiento de un experto o bien cuando el etique-
tado de cada muestra individual es impracticable. Esto ultimo ocurre por ejemplo en el caso de aplicaciones
con sensores remotos, como ser imagenes satelitales de terrenos donde seria muy costoso o imposible recoger
informacion real del tipo de suelo sensado en cada punto de las imagenes. En estos casos el proceso de disefio
requiere una primera etapa de andlisis de las estructuras presentes en los datos de entrenamiento.

3.1. Aprendizaje no supervisado y andlisis de agrupamientos

Dado un conjunto de entrenamiento suficientemente grande podemos inferir la funcién densidad de probabi-
lidad conjunta p(x) y recordando que

m

P(X) = > P(wi)p(x|wi)
i=l

podemos deducir que si la densidad conjunta es multimodal cada uno de los modos deberia corresponderse
con la distribucién condicional de cada una de las clases presentes. Por lo tanto identificando estos modos en
p(x) seria en principio posible particionar el espacio de observacién en regiones disjuntas [; ,i = 1,--- ,m

asociadas con cada una de las clases presentes.

Si las distribuciones condicionales de cada clase son normales cabria la posibilidad de recuperar los pardme-
tros de cada distribucién a partir del conjunto de entrenamiento. A partir de esto podriamos seguir con el
disefio del clasificador como se vio en la secci6n anterior. Sin embargo podemos conformarnos con recobrar

 NEW PAGE