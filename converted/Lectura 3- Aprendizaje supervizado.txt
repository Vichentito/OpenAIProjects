3. Apendizaje no supervisado 9

encontramos que é tiene distribucién Binomial con valor medio e (como se ve calculando la esperanza) y por
lo tanto é es un estimador insesgado del error medio e.

La desviacién estandar de este estimador se calcula como

oe = Var = (E(@] -2)

Nie

Sustituyendo é y desarrollando resulta

N N
1 1 , N-1 3 1
= yp » » E lnm] —e = 5 F [n"] + ee wel e)

j=l k=1

de donde la desviacién del estimador sera

Resumiendo, hemos visto que podemos estimar el error medio de clasificacién e presentandole a nuestro sis-
tema de clasificacidn un conjunto de patrones que pertenecen a clases conocidas. El error se estima contando
el nimero de discrepancias entre la clase verdadera y la etiqueta de clase asignada por el sistema, y dividiendo
finalmente este resultado entre el ntiimero de muestras en la prueba.

Notar que si el error medio del sistema es pequefio, digamos de ~ 1%, vamos a necesitar de un nimero
grande de muestras de prueba para verificar este valor de desempefio con una razonable confianza relativa.

3. Apendizaje no supervisado

Es comtn encontrarse con situaciones en las que el sistema de clasificacién de patrones debe disefiarse par-
tiendo de un conjunto de patrones de entrenamiento {a;; j = 1,2,--- , N} para los cuales no conocemos
sus etiquetas de clase ¥;-

Estas situaciones se presentan cuando no disponemos del conocimiento de un experto o bien cuando el etique-
tado de cada muestra individual es impracticable. Esto ultimo ocurre por ejemplo en el caso de aplicaciones
con sensores remotos, como ser imagenes satelitales de terrenos donde seria muy costoso o imposible recoger
informacion real del tipo de suelo sensado en cada punto de las imagenes. En estos casos el proceso de disefio
requiere una primera etapa de andlisis de las estructuras presentes en los datos de entrenamiento.

3.1. Aprendizaje no supervisado y andlisis de agrupamientos

Dado un conjunto de entrenamiento suficientemente grande podemos inferir la funcién densidad de probabi-
lidad conjunta p(x) y recordando que

m

P(X) = > P(wi)p(x|wi)
i=l

podemos deducir que si la densidad conjunta es multimodal cada uno de los modos deberia corresponderse
con la distribucién condicional de cada una de las clases presentes. Por lo tanto identificando estos modos en
p(x) seria en principio posible particionar el espacio de observacién en regiones disjuntas [; ,i = 1,--- ,m

asociadas con cada una de las clases presentes.

Si las distribuciones condicionales de cada clase son normales cabria la posibilidad de recuperar los pardme-
tros de cada distribucién a partir del conjunto de entrenamiento. A partir de esto podriamos seguir con el
disefio del clasificador como se vio en la secci6n anterior. Sin embargo podemos conformarnos con recobrar

 NEW PAGE 
10 INDICE

MY To Ts

Figura 6: Distribucion conjunta multimodal y regiones asociadas a cada clase..

directamente las regiones I’; lo cual es suficiente para nuestros intereses ya que esto puede usarse directa-
mente para la clasificacién de nuevos datos simplemente usando el criterio:

Asignarxaw; <=> x€Tj

Un opci6n alternativa seria usar este u otro criterio para clasificar los patrones en el conjunto de entrenamiento
y luego usar estas etiquetas para disefiar el sistema de reconocimiento de patrones usando un aprendizaje
supervisado. En la practica ocurre que determinar explicitamente las regiones I’; implicaria estimar la funcién
de densidad conjunta y luego analizarla en un espacio de dimensién n lo que generalmente es impracticable
por su complejidad computacional. Ademas como vimos, solo necesitamos de un método indirecto que nos
permita etiquetar automaticamente los patrones de entrenamiento. Entonces lo que queremos es alguna forma
de hacer una particién del conjunto de entrenamiento en clases con una misma etiqueta y esto es lo que se
conoce como métodos de agrupamiento 0 clustering.

Intuitivamente podemos anticipar que las modas en la funcién de densidad conjunta p(x) estaran asocia-
das a regiones con alta densidad de muestras en el espacio de observacion. El proposito de las técnicas de
agrupamiento sera justamente detectar y agrupar estos enjambres de puntos.

3.2. Medidas de Similitud y Criterios de Agrupamiento

El proposito de los métodos de agrupamiento sera analizar y extraer la estructura presente en un conjunto de
patrones o muestras de entrenamiento. Diremos que un conjunto de datos esta bien estructurado si contiene
varios enjambres de patrones cercanos entre si, o sea regiones de alta densidad, separados por otras regiones
relativamente vacias 0 con poca densidad.

Vemos que los puntos de un mismo agrupamiento apareceran mas proximos entre ellos que a puntos en otros
agrupamientos. Esta observaci6n nos lleva a concluir que si queremos decidir si un punto x pertenece o no a
un agrupamiento necesitaremos una medida de proximidad o similitud. Se han sugerido y estudiado un gran
numero de tales medidas, pero probablemente las més comunmente usadas son las medidas de distancia y en
particular la distancia Euclideana.

La afinidad de un punto a un agrupamiento se puede determinar ya sea midiendo su similitud con otros
puntos en el agrupamiento o bien con un modelo definido para el agrupamiento. El ejemplo mas sencillo de
esto ultimo es representar un agrupamiento 7 por su vector medio j1;; en este caso la afinidad entre un puntox
y el agrupamiento se puede cuantificar con la distancia Euclideana al cuadrado

d(x, pi) = [(x _ Hi)” (x - Hi)

 NEW PAGE 
3. Apendizaje no supervisado 11

@ c) ®@ @
e 4 ®
® @ e
e © ® °°
®
@ ®® &®@
®
®
® @ o ®
® ®
° ® ® ®
° ® ee ©
® ® ®
®
e °e
° ®
® ® ®
® 8 ®
o °
®
® ® s
® ®
® ®
® ee
eo ee
@®

Figura 7: Datos structurados vs. no estructurados.

Pero para particionar un conjunto de puntos en agrupamientos de una manera 6ptima no nos alcanza con una
medida de afinidad o similitud sino que ademas necesitamos algun criterio de agrupamiento que nos permita
definir cuantitativamente cuando una particién es mejor que otra. Obviamente tanto el criterio de agrupa-
miento que definamos tanto como el algoritmo de agrupamiento asociado, estaran intimamente relacionados
con la medida de similitud usada y se definiran a partir de esta.

En la siguiente seccién veremos algunos ejemplos de métodos de agrupamiento que se basan en los conceptos
anteriores.

3.3. Algoritmo de k-medias (/:-means).

Supondremos que el conjunto de datos X contiene k agrupamientos y que cada uno de estos subconjuntos
XX; puede representarse adecuadamente con su valor medio j1;. Como se menciona anteriormente, en este
caso podemos usar la distancia Euclideana como una medida de similitud. Se deduce que un criterio de
agrupamiento adecuado en este caso es considerar la suma total sobre el conjunto de entrenamiento de la
distancia cuadratica de cada punto al vector valor medio de su agrupamiento.

El objetivo del algoritmo de agrupamiento sera encontrar entre todas las particiones de X en k conjuntos
{X;; i=1,2,---,k} aquella que minimice el criterio de agrupamiento elegido.

Dicho formalmente, queremos encontrar los agrupamientos {X_;} que minimizan la funcién

k k ON;

J= SOK = SLY a xij, mi) siendo xij € Xi, Ni = #X:

i=l i=1 j=l
entre todas las posibles particiones de X en k subconjuntos.

Un algoritmo para minimizar J puede deducirse considerando el efecto de un cambio minimal 0 atémico en
la configuracién de agrupamientos, que consiste en sacar un punto x que este en el agrupamiento X, para
pasarlo a otro agrupamiento X,..

 NEW PAGE 
12 INDICE

Claramente esta reasignacion afectara solo a los agrupamientos | y r cuyos valores medios pasaran a ser

fy =m + ——(ju—X) Yip = + (u, — x)
fa = MX) YB = be ~ (Her —

1
N-1
respectivamente.

Para deducir la primera ecuaci6n calculamos el valor medio de X; antes y despues de la reasignacién

1 Ni 1 Ni-1 1 Ni
may = Ls Tl Ls *
j=l j=l j=l
donde hemos asumido que el punto reasignado es el ultimo en la sumatoria. De aqui resulta que
N, 1 1
(N; — 1)f = Ni — x i Noi No1* fi = wi 4 Noi! x)

y andlogamente se verifica la segunda identidad.

Por lo tanto para calcular el cambio global en el valor de J bastard calcular los cambios en las contribuciones
de J; y J,. Para el nuevo agrupamiento |-esimo tendremos

Ni-1 N-1
= d(xj, mu;) = > (xj — 1)" Oy — wr) =
j=l j=l
a [-X\T MW - xX ba 1 - x
= — 1 LT — 1 ‘LT — — 1 — — 4 LT
Solas —net AY me RM) — ene RAI en BF)
2 ~ M T NP T
Ti Mo z(m - ») Debs - Bi) +O ya Mt — x)" (ui — x) + w—p" — x)" (ui — x)
oe

de donde luego de agrupar concluimos que

MN
N-1

T= I- (mm — x)" (ui — x) = Ti -
y andlogamente para el agrupamiento r se obtiene

Jp. = Ip +


 NEW PAGE